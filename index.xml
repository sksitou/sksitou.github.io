<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sksitouML</title>
    <link>http://sksitou.me/index.xml</link>
    <description>Recent content on sksitouML</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 19 Feb 2017 22:00:00 +0800</lastBuildDate>
    <atom:link href="http://sksitou.me/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Machine learning notes : Least square error in linear regression</title>
      <link>http://sksitou.me/post/Machine-learning-notes-:-least-square-in-linear-regression/</link>
      <pubDate>Sun, 19 Feb 2017 22:00:00 +0800</pubDate>
      
      <guid>http://sksitou.me/post/Machine-learning-notes-:-least-square-in-linear-regression/</guid>
      <description>&lt;p&gt;Least sqaure error is used as a cost function in linear regression. However, why should one choose sqaure error, instead of absolute error, or other choices?
There&amp;rsquo;s a simple proof that can show that least sqaure error is a reasonable and natural choice.&lt;/p&gt;

&lt;p&gt;Assume the target variable and inputs are related as below:
&lt;div&gt;\(y^i=\sigma^Tx^i+\epsilon^i\)&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;,where \(\epsilon\sim\mathcal{N}(\mu,\,\sigma^{2})\)&lt;/p&gt;

&lt;p&gt;i.e.
&lt;div&gt;\(p(\epsilon^i)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^i)^2}{2\sigma^2})\)&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;implies that
&lt;div&gt;\(p(y^i|x^i_j\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^i-\theta^Tx)^2}{2\sigma^2})\)&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;We would like to minimize the error by maximising the log likelihood.
The likelihood function is
&lt;div&gt;\(L(\theta)=L(\theta:X,\vec{y})=p(y^i|x^i_j\theta)\)&lt;/div&gt;
&lt;div&gt;\(L(\theta)=\prod(y^i|x^i_j\theta)=\prod(\frac{1}{\sqrt{2\pi}}exp(-\frac{(\epsilon^i)^2}{2\sigma^2}))\)&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Minimizing the log likelihood function
&lt;div&gt;\(l(\theta)\)&lt;/div&gt;
&lt;div&gt;\(=logL(\theta)\)&lt;/div&gt;
&lt;div&gt;\(=log(\prod(\frac{1}{\sqrt{2\pi}}exp(-\frac{(\epsilon^i)^2}{2\sigma^2})))\)&lt;/div&gt;
&lt;div&gt;\(=\prod_{i=1}^{m}(log(\frac{1}{\sqrt{2\pi}}exp(-\frac{(y^i-\theta^Tx)^2}{2\sigma^2})))\)&lt;/div&gt;
&lt;div&gt;\(=mlog(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{\sigma^2}\frac{1}{2}(\sum(y^i-\theta^Tx^i)^2)\)&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;which is equivalent to minimizing
&lt;div&gt;\(\frac{1}{2}(\sum(y^i-\theta^Tx^i)^2)\)&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;, which is also known as the least sqaure function, and note that the \(\sigma^2\) is irrelavent in this case.&lt;/p&gt;

&lt;p&gt;Note that the least-sqaure method corresponds to the maximum likelihood estimation.
Hence, one can justify the least-sqaure method, with the natural assumption of \(\epsilon\sim\mathcal{N}(\mu,\,\sigma^{2})\).&lt;/p&gt;

&lt;p&gt;*It&amp;rsquo;s my first time using Katex, and it&amp;rsquo;s tough writing mathematical equations in markdown files. I chose a simple example as a practice. Here is the Github repo of Katex:
&lt;a href=&#34;https://github.com/Khan/KaTeX&#34;&gt;https://github.com/Khan/KaTeX&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>first post</title>
      <link>http://sksitou.me/post/first-post/</link>
      <pubDate>Sun, 19 Feb 2017 20:58:24 +0800</pubDate>
      
      <guid>http://sksitou.me/post/first-post/</guid>
      <description>&lt;p&gt;Hello world! It&amp;rsquo;s my first time writing articles online. Thanks for the help of my friend &lt;a href=&#34;http://carsonip.me/&#34;&gt;Carson Ip&lt;/a&gt;, who ported the library from hexo to hugo. So that I can use the framework implemented in golang, with my favourite theme minos, which is implemented in another framework called hexo. If you like this theme but want to use hugo, you may want to check out the GitHub repo: &lt;a href=&#34;https://github.com/carsonip/hugo-theme-minos&#34;&gt;https://github.com/carsonip/hugo-theme-minos&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, thanks for my friend &lt;a href=&#34;sunnynlp.me&#34;&gt;Sunny&lt;/a&gt;, who encouraged me to start this blog.
I&amp;rsquo;m going to share notes about machine learning, my pet projects and also interesting stuff here. I like learning machine learning algorithms and maths. The blog helps me to consolidate my learning. Writing them out forces me to capture them in a cohesive manner. Publishing them online makes me rethink if I have any unclear concepts.&lt;/p&gt;

&lt;p&gt;The posts are written in the format of markdown files. I found it very convenient to use markdown files. Here is the cheatsheet I used as a beginner:
&lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&#34;&gt;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>